#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM í˜¸ì¶œê¸°
ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ LLM APIë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ ì²˜ë¦¬
"""

import json
import os
import asyncio
import logging
from typing import List, Dict, Any
from pathlib import Path
from dotenv import load_dotenv

# LLM ê´€ë ¨
import google.generativeai as genai

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class LLMCaller:
    """LLM API í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬ í´ë˜ìŠ¤ (API í˜¸ì¶œ ì œí•œ ê´€ë¦¬)"""

    def __init__(self):
        # Gemini API ì„¤ì •
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.error("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise ValueError("GEMINI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        genai.configure(api_key=api_key)

        # Gemini 1.5 Pro ëª¨ë¸ ì‚¬ìš© (Flashê°€ ê³¼ë¶€í•˜ ìƒíƒœì´ë¯€ë¡œ)
        try:
            self.model = genai.GenerativeModel("gemini-1.5-pro")
            logger.info("Gemini 1.5 Pro ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Gemini 1.5 Pro ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        # API í˜¸ì¶œ ì œí•œ ê´€ë¦¬
        self.call_count = 0
        self.last_call_time = 0
        self.rate_limit_window = 60  # 60ì´ˆ ìœˆë„ìš°
        self.max_calls_per_window = 10  # 60ì´ˆë‹¹ ìµœëŒ€ 10íšŒ í˜¸ì¶œ

        logger.info("LLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _check_rate_limit(self):
        """API í˜¸ì¶œ ì œí•œ í™•ì¸"""
        import time

        current_time = time.time()

        # ìœˆë„ìš°ê°€ ì§€ë‚¬ìœ¼ë©´ ì¹´ìš´í„° ë¦¬ì…‹
        if current_time - self.last_call_time > self.rate_limit_window:
            self.call_count = 0
            self.last_call_time = current_time

        # í˜¸ì¶œ ì œí•œ í™•ì¸
        if self.call_count >= self.max_calls_per_window:
            wait_time = self.rate_limit_window - (current_time - self.last_call_time)
            if wait_time > 0:
                logger.warning(f"API í˜¸ì¶œ ì œí•œ ë„ë‹¬. {wait_time:.1f}ì´ˆ ëŒ€ê¸° í•„ìš”")
                return wait_time

        return 0

    async def call_llm(self, prompt: str) -> Dict[str, Any]:
        """
        LLM API í˜¸ì¶œ (ì§€ìˆ˜ ë°±ì˜¤í”„ + í˜¸ì¶œ ì œí•œ ì ìš©)

        Args:
            prompt (str): LLM í”„ë¡¬í”„íŠ¸

        Returns:
            Dict[str, Any]: LLM ì‘ë‹µ (íŒŒì‹±ëœ JSON ë˜ëŠ” ì˜¤ë¥˜ ì •ë³´)
        """
        max_retries = 3  # ì¬ì‹œë„ íšŸìˆ˜
        timeout = 300  # 5ë¶„ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¦ê°€
        base_delay = 2  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

        for attempt in range(max_retries):
            try:
                import asyncio
                import time

                # API í˜¸ì¶œ ì œí•œ í™•ì¸
                wait_time = self._check_rate_limit()
                if wait_time > 0:
                    logger.info(f"API í˜¸ì¶œ ì œí•œìœ¼ë¡œ ì¸í•´ {wait_time:.1f}ì´ˆ ëŒ€ê¸°...")
                    await asyncio.sleep(wait_time)

                logger.info(
                    f"LLM API í˜¸ì¶œ ì‹œë„ {attempt + 1}/{max_retries} (íƒ€ì„ì•„ì›ƒ: {timeout}ì´ˆ)"
                )

                # í˜¸ì¶œ ì¹´ìš´í„° ì¦ê°€
                self.call_count += 1

                # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë¡œê¹…
                logger.info(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")

                # Gemini 1.5 Pro API í˜¸ì¶œ ë°©ì‹
                response = await asyncio.wait_for(
                    self.model.generate_content_async(prompt), timeout=timeout
                )
                response_text = response.text

                # ì‘ë‹µ ê¸¸ì´ ë° ë‚´ìš© ë¡œê¹…
                logger.info(f"LLM ì‘ë‹µ ìˆ˜ì‹ : {len(response_text)} ë¬¸ì")
                logger.info(f"ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")

                logger.info(f"LLM API í˜¸ì¶œ ì„±ê³µ (ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì)")

                # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„ (ê°•í™”ëœ íŒŒì‹± ë¡œì§)
                try:
                    import re

                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (ì •ê·œì‹ ì‚¬ìš©)
                    cleaned_response = response_text.strip()

                    # ```json ... ``` íŒ¨í„´ ì œê±°
                    json_pattern = r"```json\s*(.*?)\s*```"
                    match = re.search(json_pattern, cleaned_response, re.DOTALL)

                    if match:
                        cleaned_response = match.group(1).strip()
                    else:
                        # ì¼ë°˜ ``` ... ``` íŒ¨í„´ ì œê±°
                        code_pattern = r"```\s*(.*?)\s*```"
                        match = re.search(code_pattern, cleaned_response, re.DOTALL)
                        if match:
                            cleaned_response = match.group(1).strip()

                    # ì¶”ê°€ ì •ë¦¬: ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì œê±°
                    cleaned_response = re.sub(r"\n\s*\n", "\n", cleaned_response)
                    cleaned_response = cleaned_response.strip()

                    # JSON íŒŒì‹± ì‹œë„
                    llm_data = json.loads(cleaned_response)
                    logger.info("JSON íŒŒì‹± ì„±ê³µ")
                    return {
                        "success": True,
                        "data": llm_data,
                        "raw_response": response_text,
                    }
                except json.JSONDecodeError as e:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê°•í™”ëœ ëŒ€ì•ˆ íŒŒì‹± ì‹œë„
                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    logger.warning(f"íŒŒì‹± ì‹œë„í•œ í…ìŠ¤íŠ¸: {cleaned_response[:200]}...")

                    # ê°•í™”ëœ JSON ì¶”ì¶œ ë° ìˆ˜ì • ì‹œë„
                    try:
                        # 1ë‹¨ê³„: ê¸°ë³¸ JSON ì¶”ì¶œ
                        json_start = cleaned_response.find("{")
                        json_end = cleaned_response.rfind("}") + 1

                        if json_start != -1 and json_end > json_start:
                            json_part = cleaned_response[json_start:json_end]

                            # 2ë‹¨ê³„: ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ìˆ˜ì •
                            # ë¶ˆì™„ì „í•œ ë¬¸ìì—´ ìˆ˜ì •
                            json_part = json_part.replace("...", "")
                            json_part = json_part.replace('"ìƒì„¸í•´ì„¤": "', '"ìƒì„¸í•´ì„¤": "')

                            # 3ë‹¨ê³„: ì¤‘ê´„í˜¸ ê· í˜• í™•ì¸ ë° ìˆ˜ì •
                            open_braces = json_part.count("{")
                            close_braces = json_part.count("}")

                            if open_braces > close_braces:
                                # ë¶€ì¡±í•œ ë‹«ëŠ” ì¤‘ê´„í˜¸ ì¶”ê°€
                                json_part += "}" * (open_braces - close_braces)
                            elif close_braces > open_braces:
                                # ë¶€ì¡±í•œ ì—¬ëŠ” ì¤‘ê´„í˜¸ ì¶”ê°€
                                json_part = (
                                    "{" * (close_braces - open_braces) + json_part
                                )

                            # 4ë‹¨ê³„: ë¶ˆì™„ì „í•œ ë¬¸ìì—´ ìˆ˜ì •
                            lines = json_part.split("\n")
                            fixed_lines = []
                            for line in lines:
                                if '"ìƒì„¸í•´ì„¤":' in line and not line.strip().endswith('"'):
                                    # ë¶ˆì™„ì „í•œ ë¬¸ìì—´ ë¼ì¸ ìˆ˜ì •
                                    line = line.strip()
                                    if not line.endswith('"'):
                                        line += '"'
                                    if not line.endswith(","):
                                        line += ","
                                fixed_lines.append(line)

                            json_part = "\n".join(fixed_lines)

                            # 5ë‹¨ê³„: ìµœì¢… JSON íŒŒì‹± ì‹œë„
                            llm_data = json.loads(json_part)
                            logger.info("ê°•í™”ëœ JSON íŒŒì‹± ì„±ê³µ")
                            return {
                                "success": True,
                                "data": llm_data,
                                "raw_response": response_text,
                            }
                    except Exception as alt_e:
                        logger.warning(f"ê°•í™”ëœ JSON íŒŒì‹±ë„ ì‹¤íŒ¨: {alt_e}")

                        # 6ë‹¨ê³„: ìµœí›„ì˜ ìˆ˜ë‹¨ - ê¸°ë³¸ êµ¬ì¡° ìƒì„±
                        try:
                            # ì‘ë‹µì—ì„œ ìƒì„¸í•´ì„¤ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                            detail_start = cleaned_response.find('"ìƒì„¸í•´ì„¤": "')
                            if detail_start != -1:
                                detail_start += len('"ìƒì„¸í•´ì„¤": "')
                                detail_end = cleaned_response.find('"', detail_start)
                                if detail_end == -1:
                                    detail_end = len(cleaned_response)

                                detail_text = cleaned_response[detail_start:detail_end]
                                if len(detail_text) > 50:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
                                    llm_data = {
                                        "ìƒì„¸í•´ì„¤": detail_text,
                                        "ì¡°ì¹˜ë°©ë²•": ["LLM ì‘ë‹µì—ì„œ ì¶”ì¶œëœ ìƒì„¸í•´ì„¤ì„ ì°¸ê³ í•˜ì„¸ìš”."],
                                    }
                                    logger.info("ê¸°ë³¸ êµ¬ì¡° ìƒì„± ì„±ê³µ")
                                    return {
                                        "success": True,
                                        "data": llm_data,
                                        "raw_response": response_text,
                                    }
                        except Exception as final_e:
                            logger.warning(f"ê¸°ë³¸ êµ¬ì¡° ìƒì„±ë„ ì‹¤íŒ¨: {final_e}")

                    return {
                        "success": False,
                        "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                        "raw_response": response_text,
                    }

            except asyncio.TimeoutError:
                logger.warning(
                    f"LLM í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{max_retries}, {timeout}ì´ˆ ì´ˆê³¼)"
                )
                if attempt == max_retries - 1:
                    return {
                        "success": False,
                        "error": f"íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ ì´ˆê³¼)",
                        "raw_response": "",
                    }
                # ì§€ìˆ˜ ë°±ì˜¤í”„: 2ì´ˆ, 4ì´ˆ, 8ì´ˆ
                delay = base_delay * (2**attempt)
                logger.info(f"ì¬ì‹œë„ ì „ {delay}ì´ˆ ëŒ€ê¸°...")
                await asyncio.sleep(delay)

            except Exception as e:
                logger.error(f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    return {"success": False, "error": str(e), "raw_response": ""}
                # ì§€ìˆ˜ ë°±ì˜¤í”„: 2ì´ˆ, 4ì´ˆ, 8ì´ˆ
                delay = base_delay * (2**attempt)
                logger.info(f"ì¬ì‹œë„ ì „ {delay}ì´ˆ ëŒ€ê¸°...")
                await asyncio.sleep(delay)

    async def process_prompts_batch(
        self, prompts: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (API í˜¸ì¶œ ë¹ˆë„ ì œí•œ)
        ì „ì²´ 36ê°œ í•­ëª© ì²˜ë¦¬

        Args:
            prompts (List[Dict[str, Any]]): ì²˜ë¦¬í•  í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[Dict[str, Any]]: ì²˜ë¦¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì „ì²´ í•­ëª© ì²˜ë¦¬ (36ê°œ)
        # prompts = prompts[:5]  # ì œí•œ í•´ì œ

        results = []
        total_prompts = len(prompts)
        batch_size = 5  # ë°°ì¹˜ í¬ê¸°ë¥¼ 5ê°œë¡œ ì„¤ì • (36ê°œ ì²˜ë¦¬ ìµœì í™”)

        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {total_prompts}ê°œ í”„ë¡¬í”„íŠ¸ (ë°°ì¹˜ í¬ê¸°: {batch_size}) - ì „ì²´ ì²˜ë¦¬ ëª¨ë“œ")

        for i, prompt_data in enumerate(prompts, 1):
            cce_id = prompt_data.get("original_item", {}).get("CCE_ID", f"Unknown-{i}")
            prompt_text = prompt_data.get("prompt", "")

            # ì§„í–‰ë¥  ê³„ì‚°
            progress = (i / total_prompts) * 100
            estimated_remaining = (total_prompts - i) * 15  # ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ (ì´ˆ)

            logger.info(f"ğŸ”„ LLM ì²˜ë¦¬ ì¤‘: {i}/{total_prompts} ({progress:.1f}%) - {cce_id}")
            logger.info(
                f"â±ï¸  ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining // 60}ë¶„ {estimated_remaining % 60}ì´ˆ"
            )

            # LLM í˜¸ì¶œ
            llm_response = await self.call_llm(prompt_text)

            # ê²°ê³¼ ì¡°í•©
            result = {
                "CCE_ID": cce_id,
                "original_data": prompt_data.get("original_item", {}),
                "llm_response": llm_response,
            }

            results.append(result)

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëŒ€ê¸° (5ê°œë§ˆë‹¤ 20ì´ˆ ëŒ€ê¸°)
            if i % batch_size == 0 and i < total_prompts:
                logger.info(f"ë°°ì¹˜ ì™„ë£Œ ({i}/{total_prompts}). 20ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ ë°°ì¹˜ ì‹œì‘...")
                await asyncio.sleep(20)
            else:
                # ê°œë³„ ìš”ì²­ ê°„ 8ì´ˆ ëŒ€ê¸° (36ê°œ ì²˜ë¦¬ ìµœì í™”)
                if i < total_prompts:
                    await asyncio.sleep(8)

        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ (ì „ì²´ ì²˜ë¦¬ ëª¨ë“œ)")
        return results

    def save_results(self, results: List[Dict[str, Any]], output_file: str):
        """
        LLM ì‘ë‹µ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥

        Args:
            results (List[Dict[str, Any]]): LLM ì‘ë‹µ ê²°ê³¼
            output_file (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            logger.info(f"LLM ì‘ë‹µ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")

        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # LLM í˜¸ì¶œê¸° ì´ˆê¸°í™”
        caller = LLMCaller()

        # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ
        prompts_file = "output/llm_prompts.json"

        if not Path(prompts_file).exists():
            logger.error(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompts_file}")
            return

        with open(prompts_file, "r", encoding="utf-8") as f:
            prompts = json.load(f)

        print(f"ğŸ“‹ {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")

        # LLM ë°°ì¹˜ ì²˜ë¦¬
        results = await caller.process_prompts_batch(prompts)

        print(f"âœ… {len(results)}ê°œ LLM ì‘ë‹µ ì²˜ë¦¬ ì™„ë£Œ")

        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        success_count = sum(1 for r in results if r["llm_response"]["success"])
        failure_count = len(results) - success_count

        print(f"ğŸ“Š ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {failure_count}")

        # ì²« ë²ˆì§¸ ì„±ê³µ ì‘ë‹µ ì˜ˆì‹œ ì¶œë ¥
        for result in results:
            if result["llm_response"]["success"]:
                print("\nğŸ“ ì²« ë²ˆì§¸ ì„±ê³µ ì‘ë‹µ ì˜ˆì‹œ:")
                print("=" * 50)
                print(
                    json.dumps(
                        result["llm_response"]["data"], ensure_ascii=False, indent=2
                    )
                )
                print("=" * 50)
                break

        # ê²°ê³¼ ì €ì¥
        output_file = "output/llm_responses.json"
        caller.save_results(results, output_file)

        print(f"\nğŸ’¾ LLM ì‘ë‹µ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")

    except Exception as e:
        logger.error(f"LLM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    asyncio.run(main())
