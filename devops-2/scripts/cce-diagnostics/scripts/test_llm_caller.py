#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM í˜¸ì¶œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ëª‡ ê°œ í•­ëª©ë§Œ í…ŒìŠ¤íŠ¸í•˜ì—¬ LLM APIê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
"""

import json
import asyncio
import logging
from pathlib import Path
from llm_caller import LLMCaller

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_llm_caller():
    """LLM í˜¸ì¶œ í…ŒìŠ¤íŠ¸"""
    try:
        # LLM í˜¸ì¶œê¸° ì´ˆê¸°í™”
        caller = LLMCaller()
        
        # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ
        prompts_file = "output/llm_prompts.json"
        
        if not Path(prompts_file).exists():
            logger.error(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompts_file}")
            return
        
        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompts = json.load(f)
        
        print(f"ğŸ“‹ {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")
        
        # ì²˜ìŒ 3ê°œ í•­ëª©ë§Œ í…ŒìŠ¤íŠ¸
        test_prompts = prompts[:3]
        print(f"ğŸ§ª ì²˜ìŒ 3ê°œ í•­ëª©ìœ¼ë¡œ LLM í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # LLM ë°°ì¹˜ ì²˜ë¦¬
        results = await caller.process_prompts_batch(test_prompts)
        
        print(f"âœ… {len(results)}ê°œ LLM ì‘ë‹µ ì²˜ë¦¬ ì™„ë£Œ")
        
        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        success_count = sum(1 for r in results if r['llm_response']['success'])
        failure_count = len(results) - success_count
        
        print(f"ğŸ“Š ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {failure_count}")
        
        # ì²« ë²ˆì§¸ ì„±ê³µ ì‘ë‹µ ì˜ˆì‹œ ì¶œë ¥
        for result in results:
            if result['llm_response']['success']:
                print("\nğŸ“ ì²« ë²ˆì§¸ ì„±ê³µ ì‘ë‹µ ì˜ˆì‹œ:")
                print("=" * 50)
                print(json.dumps(result['llm_response']['data'], ensure_ascii=False, indent=2))
                print("=" * 50)
                break
        
        # ê²°ê³¼ ì €ì¥
        output_file = "output/test_llm_responses.json"
        caller.save_results(results, output_file)
        
        print(f"\nğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        
    except Exception as e:
        logger.error(f"LLM í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(test_llm_caller()) 