#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
1ë‹¨ê³„: JSON ì§„ë‹¨ í•­ëª© ë¡œë”©
2ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
3ë‹¨ê³„: LLM API í˜¸ì¶œ
4ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° Excel ë³´ê³ ì„œ ìƒì„±
"""

import json
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_step1_load_items(input_file: str) -> List[Dict[str, Any]]:
    """1ë‹¨ê³„: JSON ì§„ë‹¨ í•­ëª© ë¡œë”©"""
    print("ğŸ”„ 1ë‹¨ê³„: JSON ì§„ë‹¨ í•­ëª© ë¡œë”© ì‹œì‘...")
    
    from load_diagnostic_items import load_diagnostic_items
    items = load_diagnostic_items(input_file)
    
    if not items:
        raise ValueError("ë¡œë“œí•  ì§„ë‹¨ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {len(items)}ê°œ í•­ëª© ë¡œë“œ")
    return items

def run_step2_generate_prompts(items: List[Dict[str, Any]], system_type: str = "Linux") -> List[Dict[str, Any]]:
    """2ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    print("ğŸ”„ 2ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œì‘...")
    
    from llm_prompt_generator import generate_batch_prompts, save_prompts_to_file
    
    prompts = generate_batch_prompts(items, system_type)
    
    # í”„ë¡¬í”„íŠ¸ ì €ì¥
    output_file = "output/llm_prompts.json"
    save_prompts_to_file(prompts, output_file)
    
    print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ìƒì„±")
    return prompts

async def run_step3_call_llm(prompts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """3ë‹¨ê³„: LLM API í˜¸ì¶œ"""
    print("ğŸ”„ 3ë‹¨ê³„: LLM API í˜¸ì¶œ ì‹œì‘...")
    
    from llm_caller import LLMCaller
    
    caller = LLMCaller()
    results = await caller.process_prompts_batch(prompts)
    
    # ê²°ê³¼ ì €ì¥
    output_file = "output/llm_responses.json"
    caller.save_results(results, output_file)
    
    # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
    success_count = sum(1 for r in results if r['llm_response']['success'])
    failure_count = len(results) - success_count
    
    print(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {failure_count}")
    return results

def run_step4_generate_excel(results: List[Dict[str, Any]], system_type: str = "Linux"):
    """4ë‹¨ê³„: Excel ë³´ê³ ì„œ ìƒì„±"""
    print("ğŸ”„ 4ë‹¨ê³„: Excel ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
    
    # LLM ì‘ë‹µì„ ì›ë³¸ ë°ì´í„°ì™€ í†µí•©
    enhanced_items = []
    
    for result in results:
        original_item = result['original_item']
        llm_response = result['llm_response']
        
        if llm_response['success']:
            # LLM ì‘ë‹µ ë°ì´í„° ì¶”ê°€
            enhanced_item = {
                **original_item,
                'ì‹œìŠ¤í…œ': system_type,
                'ìƒì„¸í•´ì„¤': llm_response['data'].get('ìƒì„¸í•´ì„¤', ''),
                'ì¡°ì¹˜ë°©ë²•': llm_response['data'].get('ì¡°ì¹˜ë°©ë²•', [])
            }
        else:
            # LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            enhanced_item = {
                **original_item,
                'ì‹œìŠ¤í…œ': system_type,
                'ìƒì„¸í•´ì„¤': 'LLM ì²˜ë¦¬ ì‹¤íŒ¨',
                'ì¡°ì¹˜ë°©ë²•': ['LLM ì²˜ë¦¬ ì‹¤íŒ¨']
            }
        
        enhanced_items.append(enhanced_item)
    
    # Excel ë³´ê³ ì„œ ìƒì„±
    from report_generator.generate_csap_excel import CSAPExcelReportGenerator
    
    generator = CSAPExcelReportGenerator()
    timestamp = generator.generate_csap_excel_from_items(enhanced_items, system_type)
    
    print(f"âœ… 4ë‹¨ê³„ ì™„ë£Œ: Excel ë³´ê³ ì„œ ìƒì„± - {timestamp}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        print("ğŸš€ LLM íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 50)
        
        # ì…ë ¥ íŒŒì¼ ì„¤ì •
        input_file = "output/linux_result.json"
        system_type = "Linux"
        
        # 1ë‹¨ê³„: JSON ì§„ë‹¨ í•­ëª© ë¡œë”©
        items = run_step1_load_items(input_file)
        
        # 2ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompts = run_step2_generate_prompts(items, system_type)
        
        # 3ë‹¨ê³„: LLM API í˜¸ì¶œ
        results = await run_step3_call_llm(prompts)
        
        # 4ë‹¨ê³„: Excel ë³´ê³ ì„œ ìƒì„±
        run_step4_generate_excel(results, system_type)
        
        print("=" * 50)
        print("ğŸ‰ LLM íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print("- output/llm_prompts.json")
        print("- output/llm_responses.json")
        print("- output/csap_linux_report_*.xlsx")
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main()) 